{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")\n",
    "import vid_utils\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np \n",
    "import gc\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self,max_frames,stride_fast,slowfast_alpha):\n",
    "        super().__init__()\n",
    "        self.max_frames = max_frames\n",
    "        self.stride_fast = stride_fast\n",
    "        self.slowfast_alpha = slowfast_alpha\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        frames = frames[:,0:self.max_frames,:,:]\n",
    "        fast_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // self.stride_fast\n",
    "            ).long(),\n",
    "        )\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            fast_pathway,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, fast_pathway.shape[1] - 1, fast_pathway.shape[1] // self.slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "class SlowfastWrapper:\n",
    "    def __init__(self):\n",
    "        self.model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "        # Check if GPU is available, else use CPU\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device).eval()\n",
    "        self.layers = [ \n",
    "                [\"slow_pathway_1\", 2, self.model.blocks[1].multipathway_blocks[0].res_blocks[-1]],\n",
    "                [\"fast_pathway_1\", 2, self.model.blocks[1].multipathway_blocks[1].res_blocks[-1]],\n",
    "                [\"slow_pathway_2\", 2, self.model.blocks[2].multipathway_blocks[0].res_blocks[-1]],\n",
    "                [\"fast_pathway_2\", 2, self.model.blocks[2].multipathway_blocks[1].res_blocks[-1]],\n",
    "                [\"slow_pathway_3\", 2, self.model.blocks[3].multipathway_blocks[0].res_blocks[-1]],\n",
    "                [\"fast_pathway_3\", 2, self.model.blocks[3].multipathway_blocks[1].res_blocks[-1]],\n",
    "                [\"slow_pathway_4\", 2, self.model.blocks[4].multipathway_blocks[0].res_blocks[-1]],\n",
    "                [\"fast_pathway_4\", 2, self.model.blocks[4].multipathway_blocks[1].res_blocks[-1]],\n",
    "                [\"block_5\", -1, self.model.blocks[5]],\n",
    "                [\"block_6_proj\", -1, self.model.blocks[6].proj]\n",
    "        ]\n",
    "        self.layer_names = [layer[0] for layer in self.layers]\n",
    "        self.batch_model = False\n",
    "        \n",
    "    def preprocess_algonauts_video(self, video_path):\n",
    "        side_size = 256\n",
    "        mean = [0.45, 0.45, 0.45]\n",
    "        std = [0.225, 0.225, 0.225]\n",
    "        crop_size = 256\n",
    "        max_frames = 64\n",
    "        frames_per_second = 30\n",
    "        stride_slow = 8\n",
    "        slowfast_alpha = 4\n",
    "        stride_fast = stride_slow//slowfast_alpha\n",
    "        transform =  ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(max_frames),\n",
    "                    Lambda(lambda x: x/255.0),\n",
    "                    NormalizeVideo(mean, std),\n",
    "                    ShortSideScale(\n",
    "                        size=side_size\n",
    "                    ),\n",
    "                    CenterCropVideo(crop_size),\n",
    "                    PackPathway(max_frames, stride_fast, slowfast_alpha)\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        start_sec = 0\n",
    "        video = {'video': vid_utils.read_mp4_video(video_path)}\n",
    "        video_data = transform(video)\n",
    "        inputs = video_data[\"video\"]\n",
    "        inputs = [i.to(self.device)[None, ...] for i in inputs]\n",
    "        return inputs\n",
    "    \n",
    "    def get_activations(self, video_path, flatten = True):\n",
    "        activations_dir = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activations_dir[name] = output.detach().cpu().numpy()\n",
    "            return hook\n",
    "        # Set up hooks for getting the activations\n",
    "        handles = []\n",
    "        for layer in self.layers:\n",
    "            handle = layer[-1].register_forward_hook(get_activation(layer[0]))\n",
    "            handles.append(handle)\n",
    "        inputs = self.preprocess_algonauts_video(video_path)\n",
    "        _ = self.model(inputs)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if type(layer[1]) == list or layer[1] > 0:\n",
    "                activations_dir[layer[0]] =activations_dir[layer[0]].mean(axis=layer[1])\n",
    "            if flatten:\n",
    "                activations_dir[layer[0]]=activations_dir[layer[0]].flatten()\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        return activations_dir\n",
    "\n",
    "def get_model_names():\n",
    "    return [\"slowfast\"]\n",
    "\n",
    "def select_model(model_name):\n",
    "    if model_name == \"slowfast\":\n",
    "        return SlowfastWrapper()\n",
    "\n",
    "def get_all_activations_and_save(wrapped_model, video_list, activations_folder):\n",
    "    for video_file in tqdm(video_list):\n",
    "        video_file_name = os.path.split(video_file)[-1].split(\".\")[0]\n",
    "        activations = wrapped_model.get_activations(video_file, True)\n",
    "    \n",
    "        for layer in list(activations.keys()):\n",
    "            save_path = os.path.join(activations_folder, video_file_name+\"_\"+\"layer\" + \"_\" + str(layer) + \".npy\")\n",
    "            np.save(save_path,activations[layer])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T23:38:07.627154800Z",
     "start_time": "2023-12-03T23:38:07.618352600Z"
    }
   },
   "id": "27aaac42c19b2466"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\andre/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
      "100%|██████████| 300/300 [00:45<00:00,  6.65it/s]\n",
      "100%|██████████| 300/300 [00:41<00:00,  7.15it/s]\n",
      "100%|██████████| 300/300 [00:40<00:00,  7.37it/s]\n",
      "100%|██████████| 202/202 [00:28<00:00,  7.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 2: Specify input and output folders\n",
    "input_folder = r\"C:\\Users\\andre\\OneDrive\\Documents\\GitHub\\Brainvision_Project\\Motion_files\\videos_processed\"\n",
    "output_folder = r\"C:\\Users\\andre\\OneDrive\\Documents\\GitHub\\Brainvision_Project\\Motion_files\\Layers-features\\videos\"\n",
    "\n",
    "# Step 3: Get a list of video files in the input folder\n",
    "video_list = [os.path.join(input_folder, filename) for filename in os.listdir(input_folder) if filename.endswith(\".mp4\")]\n",
    "\n",
    "# Step 4: Initialize the selected model\n",
    "selected_model = select_model(\"slowfast\")\n",
    "\n",
    "# Step 5: Set the batch size for processing\n",
    "batch_size = 300\n",
    "\n",
    "# Step 6: Iterate through video files, compute activations, and save them in batches\n",
    "for i in range(0, len(video_list), batch_size):\n",
    "    batch_video_list = video_list[i:i+batch_size]\n",
    "\n",
    "    # Call your function to process the batch of videos\n",
    "    get_all_activations_and_save(selected_model, batch_video_list, output_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T23:43:14.935823900Z",
     "start_time": "2023-12-03T23:40:37.075953800Z"
    }
   },
   "id": "d7c0785a0c4c9955"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "directory = r\"C:\\Users\\andre\\OneDrive\\Documents\\GitHub\\Brainvision_Project\\Motion_files\\Layers-features\\opticalflow\"  # Replace with your directory path\n",
    "new_text = 'flow'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        new_filename = filename.replace('.', f'{new_text}.')\n",
    "        os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T23:46:06.339787100Z",
     "start_time": "2023-12-03T23:46:03.921246100Z"
    }
   },
   "id": "35a90362e4782037"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
