{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb579d9c64e8fbff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Channel PCA captures most significant patterns across frames for each channel, potentially highlighting the most prominent changes or features in the video content for that specific channel.\n",
    "**Do we want that or are there better ways to capture spatial features per frame?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9faca7027604e2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.092361800Z",
     "start_time": "2023-12-28T12:55:11.933145900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55940dce8be960b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.092361800Z",
     "start_time": "2023-12-28T12:55:13.087054800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Root: C:\\Users\\marce\\PycharmProjects\\Brainvision_Project\n"
     ]
    }
   ],
   "source": [
    "def find_repo_root(path='.'):\n",
    "    path = os.path.abspath(path)\n",
    "    while not os.path.isdir(os.path.join(path, '.git')):\n",
    "        parent = os.path.dirname(path)\n",
    "        if parent == path:\n",
    "            # We've reached the root of the file system without finding '.git'\n",
    "            return None\n",
    "        path = parent\n",
    "    return path\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "print(\"Repository Root:\", repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295ff56dbac193c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.110727900Z",
     "start_time": "2023-12-28T12:55:13.092361800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_full_path(relative_path, repo_root):\n",
    "    if not repo_root:\n",
    "        raise ValueError(\"Repository root not found. Ensure you're inside a Git repository.\")\n",
    "\n",
    "    return os.path.join(repo_root, relative_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d0d16c1636fca7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.118742500Z",
     "start_time": "2023-12-28T12:55:13.102753Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 1 function\n",
    "def load_and_combine_tensors(stage_name, input_folder, num_videos):\n",
    "    combined_tensor = []\n",
    "    video_indices = {}\n",
    "\n",
    "    for video_id in range(1, num_videos + 1):\n",
    "        filename = f\"{str(video_id).zfill(4)}_{stage_name}.pkl\"\n",
    "        file_path = os.path.join(input_folder, stage_name, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            #print(f\"Loading tensor from: {file_path}\")\n",
    "            with open(file_path, 'rb') as file:\n",
    "                tensor = pickle.load(file)\n",
    "                combined_tensor.append(tensor)\n",
    "                # Track start and end indices for each video\n",
    "                end_index = sum(t.shape[0] for t in combined_tensor)\n",
    "                video_indices[str(video_id).zfill(4)] = (end_index - tensor.shape[0], end_index)\n",
    "\n",
    "    if not combined_tensor:\n",
    "        print(\"No tensors found to combine.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_tensor = np.concatenate(combined_tensor, axis=0)\n",
    "    return combined_tensor, video_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698222bfb9b8029c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.166140700Z",
     "start_time": "2023-12-28T12:55:13.118742500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: globalized standardization (only based on training set)\n",
    "def standardize_tensors(combined_tensor, video_indices, training_end_id='0005'):\n",
    "    reshaped_tensor = combined_tensor.reshape(combined_tensor.shape[0], -1)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Find the end index of the training set\n",
    "    training_end_index = video_indices[training_end_id][1]\n",
    "\n",
    "    # Fit the scaler only on the training set\n",
    "    scaler.fit(reshaped_tensor[:training_end_index])\n",
    "\n",
    "    # Transform both training and test sets\n",
    "    standardized_data = scaler.transform(reshaped_tensor)\n",
    "    \n",
    "    return standardized_data.reshape(combined_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7243f3621ad841",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.166140700Z",
     "start_time": "2023-12-28T12:55:13.121099400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Separate the standardized tensor back into individual tensors\n",
    "def separate_standardized_tensor(standardized_tensor, video_indices):\n",
    "    separated_tensors = {}\n",
    "    for video_id, (start, end) in video_indices.items():\n",
    "        separated_tensors[video_id] = standardized_tensor[start:end, :]\n",
    "    return separated_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28218b83061c622",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.166140700Z",
     "start_time": "2023-12-28T12:55:13.134849400Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get same number of PCs for each video: fit PCA on all videos with a given variance threshold. Find max number of components. Fit and transform PCA with max number of components. -> makes sure that variance captured in each video is >= variance_ratio.\n",
    "\n",
    "def apply_fmpca_and_save(separated_tensors, stage_name, output_folder, variance_ratio):\n",
    "    # Assuming 'separated_tensors' is your dictionary with video IDs as keys and feature maps as values\n",
    "    print(f\"Variance Ratio: {variance_ratio}\")\n",
    "    \n",
    "    # Initialize a dictionary to store the final PCA results for each video\n",
    "    final_pca_results = {}\n",
    "    # Determine the spatial dimensions product from the first tensor\n",
    "    first_feature_map = next(iter(separated_tensors.values()))\n",
    "    spatial_dims_product = np.prod(first_feature_map.shape[2:4])  # Assuming spatial dimensions are in 3rd and 4th place\n",
    "    \n",
    "    # Step 1: Determine the maximum number of components needed, but only for training set\n",
    "    max_components = 0\n",
    "    training_video_ids = [vid for vid in separated_tensors if vid <= \"0800\"]\n",
    "    for video_id in tqdm(training_video_ids, desc=\"Finding max. number of PCs...\"):\n",
    "        feature_maps = separated_tensors[video_id]\n",
    "        for channel in range(feature_maps.shape[-1]):\n",
    "            pca = PCA(n_components=variance_ratio)\n",
    "            data_for_channel = feature_maps[..., channel].reshape(-1, spatial_dims_product)\n",
    "            pca.fit(data_for_channel)\n",
    "            max_components = max(max_components, pca.n_components_)\n",
    "    \n",
    "    print(f\"Max. number of PCs: {max_components}\")\n",
    "    \n",
    "    # Step 2: Apply PCA with the determined number of components\n",
    "    for video_id, feature_maps in tqdm(separated_tensors.items(), desc=\"Performing PCA...\"):\n",
    "        pca_results = []\n",
    "    \n",
    "        # Loop over each channel\n",
    "        for channel in range(feature_maps.shape[-1]):\n",
    "            # Reshape the data for this channel\n",
    "            data_for_pca = feature_maps[..., channel].reshape(-1, spatial_dims_product)\n",
    "    \n",
    "            # Apply PCA with the maximum number of components\n",
    "            pca = PCA(n_components=max_components)\n",
    "            pca_result = pca.fit_transform(data_for_pca)\n",
    "    \n",
    "            pca_results.append(pca_result)\n",
    "    \n",
    "        # Concatenate the PCA results from all channels for this video\n",
    "        final_result = np.concatenate(pca_results, axis=1)\n",
    "    \n",
    "        # Store the result in the dictionary with the video ID as the key\n",
    "        final_pca_results[video_id] = final_result\n",
    "        # final_pca_results now contains the PCA-transformed data for each video\n",
    "        # print(f\"Processed Video ID: {video_id}, Resulting Shape: {final_result.shape}\")\n",
    "    print(variance_ratio)\n",
    "    pca_folder = os.path.join(output_folder, f\"PCA_channel_{variance_ratio}\", stage_name)\n",
    "    if not os.path.exists(pca_folder):\n",
    "        os.makedirs(pca_folder)\n",
    "        \n",
    "    file_path = os.path.join(pca_folder, 'pca_results.pkl')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(final_pca_results, f)\n",
    "    \n",
    "    print(f\"{stage_name} PCs stored in: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273094de-3d09-432e-b631-77f632922d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:13.894992400Z",
     "start_time": "2023-12-28T12:55:13.879325700Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_stage_for_pca(input_folder, output_folder, stage_name):\n",
    "    \"\"\"\n",
    "    Process all videos of a given stage: standardize, apply PCA, and save the PCA-transformed tensors.\n",
    "    Args:\n",
    "    - input_folder: Folder containing the pre-processed videos.\n",
    "    - output_folder: Folder to save PCA results.\n",
    "    - stage_name: Name of the stage to process.\n",
    "    Returns:\n",
    "    - DataFrame containing metadata (video ID and variance captured).\n",
    "    \"\"\"\n",
    "     # Use the current working directory or a known absolute path\n",
    "    current_working_directory = os.getcwd()\n",
    "    stage_folder = os.path.join(current_working_directory, input_folder, stage_name)\n",
    "    print(\"Attempting to access:\", stage_folder)\n",
    "\n",
    "    if not os.path.exists(stage_folder):\n",
    "        print(\"Directory not found:\", stage_folder)\n",
    "        return None\n",
    "    # Calculate the number of video files in the folder\n",
    "    num_videos = len([f for f in os.listdir(stage_folder) if os.path.isfile(os.path.join(stage_folder, f))])\n",
    "    print(f\"Number of videos found: {num_videos}\")\n",
    "\n",
    "    # Step 1: Load and combine tensors\n",
    "    combined_tensor, video_indices = load_and_combine_tensors(stage_name, input_folder, num_videos)\n",
    "    print(\"Step 1 done.\")\n",
    "    # Step 2: Globally standardize the tensor\n",
    "    standardized_tensor = standardize_tensors(combined_tensor, video_indices)\n",
    "    print(\"Step 2 done.\")\n",
    "    # Step 3: Separate the standardized tensor back into individual tensors\n",
    "    separated_tensors = separate_standardized_tensor(standardized_tensor, video_indices)\n",
    "    print(\"Step 3 done.\")\n",
    "    # Step 4: Apply PCA to each tensor and save the result\n",
    "    apply_fmpca_and_save(separated_tensors, stage_name, output_folder, variance_ratio)\n",
    "    print(\"Step 4 done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a2182002e8f9b0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:15.182142400Z",
     "start_time": "2023-12-28T12:55:15.166118400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\PycharmProjects\\Brainvision_Project\n",
      "C:\\Users\\marce\\PycharmProjects\\Brainvision_Project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc8f4a7dfe583730",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:19.489287800Z",
     "start_time": "2023-12-28T12:55:19.488772900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_folder = 'preprocessed_videos_30frames'\n",
    "output_folder = os.getcwd()\n",
    "stages = [\"stage_1\", \"stage_2\", \"stage_3\", \"stage_4\", \"stage_5\"]\n",
    "variance_ratio = 0.95\n",
    "\n",
    "# Iterate over each stage and process it\n",
    "# for stage in stages:\n",
    "#     print(f\"Processing {stage}...\")\n",
    "#     process_stage_for_pca(input_folder, output_folder, stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "combined_tensor, video_indices = load_and_combine_tensors(\"stage_5\", input_folder, 5)\n",
    "standardized_tensor = standardize_tensors(combined_tensor, video_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:22.350405700Z",
     "start_time": "2023-12-28T12:55:22.051689Z"
    }
   },
   "id": "4fc1d6ca5308029a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(standardized_tensor.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:22.423135900Z",
     "start_time": "2023-12-28T12:55:22.407049600Z"
    }
   },
   "id": "610ebd82971975fe"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Function to slice feature maps into four equal parts\n",
    "def slice_feature_maps(feature_maps):\n",
    "    _, _, height, width, _ = feature_maps.shape\n",
    "\n",
    "    # Calculate midpoints\n",
    "    mid_height = height // 2\n",
    "    mid_width = width // 2\n",
    "\n",
    "    # Slicing the feature maps into four equal parts\n",
    "    top_left = feature_maps[:, :, :mid_height, :mid_width, :]\n",
    "    top_right = feature_maps[:, :, :mid_height, mid_width:, :]\n",
    "    bottom_left = feature_maps[:, :, mid_height:, :mid_width, :]\n",
    "    bottom_right = feature_maps[:, :, mid_height:, mid_width:, :]\n",
    "\n",
    "    return top_left, top_right, bottom_left, bottom_right"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:23.808733200Z",
     "start_time": "2023-12-28T12:55:23.790665Z"
    }
   },
   "id": "c6666d9a44e22014"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Apply the function to your feature maps\n",
    "top_left, top_right, bottom_left, bottom_right = slice_feature_maps(standardized_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:24.543042200Z",
     "start_time": "2023-12-28T12:55:24.527339600Z"
    }
   },
   "id": "94d876564263a863"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1, 3, 3, 2048)\n",
      "(150, 1, 3, 4, 2048)\n",
      "(150, 1, 4, 3, 2048)\n",
      "(150, 1, 4, 4, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(top_left.shape)\n",
    "print(top_right.shape)\n",
    "print(bottom_left.shape)\n",
    "print(bottom_right.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:25.001857900Z",
     "start_time": "2023-12-28T12:55:24.992416900Z"
    }
   },
   "id": "39d2fdaaf05365d2"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def apply_pca_and_concatenate(slices, n_components=0.95):\n",
    "    pca_results = []\n",
    "    \n",
    "    for feature_maps in slices:\n",
    "        # Reshape the slice to 2D array\n",
    "        # Reshape and concatenate the feature maps for each 30-frame segment\n",
    "        n_videos = feature_maps.shape[0] // 30\n",
    "        reshaped_data = np.zeros((n_videos, feature_maps.shape[2] * feature_maps.shape[3] * feature_maps.shape[4] * 30))\n",
    "        \n",
    "        for i in range(n_videos):\n",
    "            # Flatten and concatenate the feature maps for each segment\n",
    "            video = feature_maps[i*30:(i+1)*30].reshape(-1)\n",
    "            reshaped_data[i, :] = video\n",
    "\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_result = pca.fit_transform(reshaped_data)\n",
    "        \n",
    "        # Append the PCA result\n",
    "        pca_results.append(pca_result)\n",
    "\n",
    "    # Concatenate the PCA results from all slices\n",
    "    final_result = np.concatenate(pca_results, axis=1)\n",
    "    \n",
    "    return final_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:25.618871400Z",
     "start_time": "2023-12-28T12:55:25.615561100Z"
    }
   },
   "id": "9fa3a70ead172079"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "slices = [top_left, top_right, bottom_left, bottom_right]\n",
    "final_pca_result = apply_pca_and_concatenate(slices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:28.178872800Z",
     "start_time": "2023-12-28T12:55:26.642221100Z"
    }
   },
   "id": "cc728c6c646e6563"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 16)\n",
      "[[ -61.67743206 -182.72352141 -190.36957254  391.67921109  -93.03807272\n",
      "   -60.96527269  319.69159956  438.04435365  817.19379987  -84.17123842\n",
      "   -64.99054362  -12.84645103 -300.750603   -263.12937379  701.74759977\n",
      "  -250.92240277]\n",
      " [ 718.69830973  340.45873959   42.02717849  -17.60801766 -437.71127115\n",
      "   684.33695898 -258.990001    -37.94636295 -195.67801175  -89.07605754\n",
      "    95.64201766  684.95452296 -284.16487703  776.34687675 -135.89779155\n",
      "  -215.70089647]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(final_pca_result.shape)\n",
    "print(final_pca_result[:2])\n",
    "print(final_pca_result[0, np.where(final_pca_result[0] == final_pca_result[1])[0]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T12:55:28.190732100Z",
     "start_time": "2023-12-28T12:55:28.178872800Z"
    }
   },
   "id": "4b3af35d77bbdc40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# when we do pca on the whole dataset, i.e. not splitting the first dim, then we are back at the initial approach (1000, flattened FM stack per video) and can create up to 800 PCs, right?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4410bc1faffb178c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd158bb567faf7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616d500-552d-47d7-9c62-96ed13a329b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip pca folder\n",
    "directory_to_zip = \"PCA_channel_0.95\"  # Replace with your directory name\n",
    "output_filename = \"PCA_channel_0.95_dataset\"  # Replace with your desired output name\n",
    "output_path = os.path.join(os.getcwd(), output_filename)\n",
    "shutil.make_archive(output_path, 'zip', directory_to_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
