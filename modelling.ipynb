{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c3b08cdd4f007a4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\julia\\anaconda3\\envs\\AdvancedML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ParameterGridgit' from 'sklearn.model_selection' (C:\\Users\\julia\\anaconda3\\envs\\AdvancedML\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EarlyStopping\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregularizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m l2\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParameterGridgit\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'ParameterGridgit' from 'sklearn.model_selection' (C:\\Users\\julia\\anaconda3\\envs\\AdvancedML\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from utils import find_repo_root, load_dict, calculate_vectorized_correlation, get_fmri, get_pca, correlation_metric, download_fmri\n",
    "from evaluation_utils import run_evaluation_pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import ParameterGrid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T09:08:38.907125800Z",
     "start_time": "2024-01-03T09:08:21.033619800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cef70d0abaf40bf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(layer, sub, ROI, X_train, X_val, y_train, y_val, hp_combinations,  visualize_results = True):\n",
    "    \"\"\"\n",
    "    conducts the training for a particular input layer of the CNN, subject & ROI. saves model.\n",
    "    :param X_train: training data (feature map PCs from first 800 videos from a particular layer)\n",
    "    :param X_val: validation data (feature map PCs from videos 801-900 from a particular layer)\n",
    "    :param y_train: training labels (scans for first 800 videos for the particular subject & ROI)\n",
    "    :param y_val: training labels (scans for videos 801-900 for the particular subject & ROI)\n",
    "    :param hp_combinations: dict with all parameter values in HPO. If hpo=False in run_training_pipeline, this only contains one single set of HP values\n",
    "    :param visualize_results: whether training will be plotted\n",
    "    :return: model parameters\n",
    "    \"\"\"\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "    print(\"Y_val shape: \", y_val.shape)\n",
    "    \n",
    "    # apply training to every HP combination. If hpo == False, only one HP combination will be checked\n",
    "    for hp_set in hp_combinations:\n",
    "        \n",
    "        # save all HP values of current iteration as variable\n",
    "        num_hidden_layers = hp_set[\"num_hidden_layers\"]\n",
    "        l2_reg = hp_set[\"l2_reg\"]\n",
    "        learning_rate = hp_set[\"learning_rate\"]\n",
    "        dropout = hp_set[\"dropout\"]\n",
    "        num_epochs = hp_set[\"num_epochs\"]\n",
    "        \n",
    "        # specify number of neurons per layer, depending on the number of inputs from a particular layer and number of output voxels\n",
    "        # leads to symmetric funnel-like shape of the network, with increasing layer sizes for WB and decreasing size for all ROIs\n",
    "        if num_hidden_layers == 1:\n",
    "            input_neurons = X_train.shape[1]\n",
    "            hidden1_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(1/2)\n",
    "            output_neurons = y_train.shape[1]\n",
    "        elif num_hidden_layers == 2:\n",
    "            input_neurons = X_train.shape[1]\n",
    "            hidden1_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(2/3)\n",
    "            hidden2_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(1/3)\n",
    "            output_neurons = y_train.shape[1]\n",
    "        \n",
    "        # model construction: 1 or 2 hidden layers. L2 Reg & Dropout.\n",
    "        if num_hidden_layers == 1:\n",
    "            model = Sequential([\n",
    "                Dense(hidden1_neurons, input_shape=(input_neurons,),\n",
    "                      activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(output_neurons, activation='linear',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout)\n",
    "            ])\n",
    "        elif num_hidden_layers == 2:\n",
    "            model = Sequential([\n",
    "                Dense(hidden1_neurons, input_shape=(input_neurons,),\n",
    "                      activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(hidden2_neurons, activation='relu',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(output_neurons, activation='linear',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout)\n",
    "            ])\n",
    "            \n",
    "        # early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "        \n",
    "        # Compiling the model\n",
    "        adam = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "        \n",
    "        # Training the model\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            callbacks=[early_stopping])\n",
    "        \n",
    "        if visualize_results:\n",
    "            # print layer overview\n",
    "            print(model.summary())\n",
    "            \n",
    "            # Plot training & validation loss values\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('Model loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "            plt.show()\n",
    "        \n",
    "        # save the model in a new folder\n",
    "        models_dir = os.path.join(os.getcwd(), \"models\", layer, ROI, sub)\n",
    "        if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "        model_name = f\"model_hidden_{num_hidden_layers}_lr_{learning_rate}_dropout_{dropout}_l2_{l2_reg}\" + \".keras\"\n",
    "        model_path = os.path.join(models_dir, model_name)\n",
    "        print(\"Saving model...\")\n",
    "        print(model_path)\n",
    "        model.save(model_path)\n",
    "        print(\"Model saved.\")\n",
    "        \n",
    "        # save the model history\n",
    "        history_dir = os.path.join(os.getcwd(), \"model_histories\", layer, ROI, sub)\n",
    "        if not os.path.exists(history_dir):\n",
    "            os.makedirs(history_dir)\n",
    "        history_name = model_name.replace('.keras', '.history')\n",
    "        history_path = os.path.join(history_dir, history_name)\n",
    "        print(\"Saving training history...\")\n",
    "        with open(history_path, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        print(\"Training history saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "101a27a9f9db4c86"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def run_training_pipeline(hpo=False, import_type=\"direct\"):\n",
    "    \"\"\"\n",
    "    conducts training. Models will be saved in .keras format for evaluation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hpo: if set to true, hyperparameter optimization will be conducted. Otherwise, a single set of hyperparameters will be applied. Adjust HP for that manually here.\n",
    "    import_type: set to \"direct\" if the full PCA output from the main Resnet is directly loaded into the CWD.\n",
    "                 set to \"indirect\" if a folder structure with one file per video is loaded.\n",
    "    \"\"\"\n",
    "    # load one only one main PCA file into the Ucloud session. This will determine the layer\n",
    "    layer_list = [\"stage_1\", \"stage_2\", \"stage_3\", \"stage_4\", \"stage_5\"]\n",
    "    for i in layer_list:\n",
    "        if os.path.exists(f\"{i}_pca.pkl\") or os.path.exists(f\"{i}_pca.npy\"):\n",
    "            layer = i\n",
    "            break\n",
    "\n",
    "    # filter out subs / ROIs manually in case not everything should be run\n",
    "    # subs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]\n",
    "    subs = [\"sub01\", \"sub02\", \"sub03\", \"sub07\", \"sub10\"]\n",
    "    ROIs = [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
    "    \n",
    "    # unless already downloaded, download the fmri data\n",
    "    download_fmri()\n",
    "    \n",
    "    # Load activations (PCA outputs)\n",
    "    train_pca,val_pca = get_pca(layer, mode=\"val\", import_type=import_type)\n",
    "    \n",
    "    # specify hyperparameter grid\n",
    "    param_grid ={\"learning_rate\": [0.0001],\n",
    "                \"num_hidden_layers\": [1,2],\n",
    "                \"dropout\": [0.2, 0.4],\n",
    "                \"l2_reg\":[0.001, 0.0001, 0.00001],\n",
    "                \"num_epochs\": [100]}\n",
    "    hp_combinations = ParameterGrid(param_grid)\n",
    "    if hpo:\n",
    "        # choose hp settings to test in current session\n",
    "        hp_combinations = list(hp_combinations)[0:4]\n",
    "    else:\n",
    "        # if no HPO, specify hyperparameter settings manually here - one value per HP\n",
    "        param_grid = {\"learning_rate\": [0.0001],\n",
    "                     \"num_hidden_layers\": [2],\n",
    "                     \"dropout\": [0.2],\n",
    "                     \"l2_reg\":[0.0001],\n",
    "                     \"num_epochs\": [100]}\n",
    "        hp_combinations = ParameterGrid(param_grid)\n",
    "        \n",
    "    for sub in subs:\n",
    "      for ROI in ROIs:\n",
    "        print (\"Starting ROI: \", ROI, \"sub: \",sub)\n",
    "        # Load fMRI data (labels)\n",
    "        if ROI == \"WB\":\n",
    "          track=\"full_track\"\n",
    "        else:\n",
    "          track=\"mini_track\"\n",
    "        fmri_train, fmri_val= get_fmri(ROI, track, sub, mode=\"val\")\n",
    "    \n",
    "        # model training\n",
    "        train_model(layer=layer,\n",
    "                    sub=sub,\n",
    "                    ROI=ROI,\n",
    "                    X_train=train_pca,\n",
    "                    X_val=val_pca,\n",
    "                    y_train=fmri_train,\n",
    "                    y_val=fmri_val,\n",
    "                    hp_combinations=hp_combinations,\n",
    "                    visualize_results=True)\n",
    "        \n",
    "        print (\"Completed ROI: \", ROI, \"sub: \",sub)\n",
    "        print(\"----------------------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:29:55.433057400Z",
     "start_time": "2023-12-13T10:29:55.433057400Z"
    }
   },
   "id": "40072c28e4bcaf4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Training & Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e45789c2c8d7894d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# conduct training & evaluation\n",
    "# if evaluation should be started before all models are trained (with all models that have been trained so far), this can be done via simply calling \"run evaluation pipeline\" separately (via evaluation.ipynb)\n",
    "if __name__ == \"__main__\":\n",
    "    if tf.test.is_gpu_available():\n",
    "        with tf.device('/GPU:0'):\n",
    "            run_training_pipeline(hpo=True)\n",
    "            run_evaluation_pipeline(data_mode=\"val\")\n",
    "    else:\n",
    "        run_training_pipeline(hpo=True)\n",
    "        run_evaluation_pipeline(data_mode=\"val\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16842abb91be7006"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summarize Training Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f610702f7d4e36ca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# \n",
    "# # Define the file name you're searching for\n",
    "# file_name = 'test_results_aggregated.csv'\n",
    "# \n",
    "# # Create an empty list to store dataframes\n",
    "# dataframes = []\n",
    "# \n",
    "# # Walk through the folder structure\n",
    "# for root, dirs, files in os.walk(os.getcwd()):\n",
    "#     for file in files:\n",
    "#         if file == file_name:\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             # Read the CSV file and append it to the list of dataframes\n",
    "#             dataframes.append(pd.read_csv(file_path))\n",
    "# \n",
    "# # Concatenate all dataframes in the list\n",
    "# concatenated_df = pd.concat(dataframes, ignore_index=True)\n",
    "# \n",
    "# concatenated_df.to_csv('concatenated_data.csv', index=False) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5140b18024ba06e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # load all training results from all separate server sessions into a single folder\n",
    "# results_path = \"validation_results\"\n",
    "# \n",
    "# aggregated_results = [file for file in os.listdir(results_path) if \"aggregated\" in file]\n",
    "# \n",
    "# results_list =[]\n",
    "# for file_name in aggregated_results:\n",
    "#     file_path = os.joindir(results_path, file_name)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     results_list.append(df)\n",
    "# \n",
    "# all_results = pd.concat(results_list, ignore_index=True)\n",
    "#     \n",
    "# \n",
    "# # Get a list of all CSV files in the folder\n",
    "# csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "# \n",
    "# # Initialize an empty list to store DataFrames\n",
    "# dataframes = []\n",
    "# \n",
    "# # Read each CSV file and append its content to the list\n",
    "# for file in csv_files:\n",
    "#     file_path = os.path.join(folder_path, file)\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     dataframes.append(df)\n",
    "# \n",
    "# # Concatenate all DataFrames into a single DataFrame\n",
    "# combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "# \n",
    "# combined_df.to_csv(\"all_validation_results.csv\", index=False)\n",
    "# \n",
    "# # Group by ROI and get the row with the maximum correlation_score in each group\n",
    "# best_models = combined_df.loc[combined_df.groupby('ROI')['correlation_score'].idxmax()]\n",
    "# \n",
    "# print(best_models)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "347ca6bdab4968"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
