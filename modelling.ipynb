{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:04:58.095363500Z",
     "start_time": "2023-12-01T15:04:58.089222300Z"
    }
   },
   "outputs": [],
   "source": [
    "from fmri_preprocessing import vectorized_correlation\n",
    "from utils import find_repo_root\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cef70d0abaf40bf4"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        ret_di = u.load()\n",
    "    return ret_di"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:04:58.979137300Z",
     "start_time": "2023-12-01T15:04:58.969388800Z"
    }
   },
   "id": "5f6f2b3be460cbb0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fast linear regression function with Pytorch (taken from CCN2021_Algonauts.ipynb)\n",
    "# class OLS_pytorch(object):\n",
    "#     def __init__(self,use_gpu=False):\n",
    "#         self.coefficients = []\n",
    "#         self.use_gpu = use_gpu\n",
    "#         self.X = None\n",
    "#         self.y = None\n",
    "# \n",
    "#     def fit(self,X,y):\n",
    "#         if len(X.shape) == 1:\n",
    "#             X = self._reshape_x(X)\n",
    "#         if len(y.shape) == 1:\n",
    "#             y = self._reshape_x(y)\n",
    "# \n",
    "#         X =  self._concatenate_ones(X)\n",
    "# \n",
    "#         X = torch.from_numpy(X).float()\n",
    "#         y = torch.from_numpy(y).float()\n",
    "#         if self.use_gpu:\n",
    "#             X = X.cuda()\n",
    "#             y = y.cuda()\n",
    "#         XtX = torch.matmul(X.t(),X)\n",
    "#         Xty = torch.matmul(X.t(),y.unsqueeze(2))\n",
    "#         XtX = XtX.unsqueeze(0)\n",
    "#         XtX = torch.repeat_interleave(XtX, y.shape[0], dim=0)\n",
    "#         betas_cholesky, _ = torch.solve(Xty, XtX)\n",
    "# \n",
    "#         self.coefficients = betas_cholesky\n",
    "# \n",
    "#     def predict(self, entry):\n",
    "#         if len(entry.shape) == 1:\n",
    "#             entry = self._reshape_x(entry)\n",
    "#         entry =  self._concatenate_ones(entry)\n",
    "#         entry = torch.from_numpy(entry).float()\n",
    "#         if self.use_gpu:\n",
    "#             entry = entry.cuda()\n",
    "#         prediction = torch.matmul(entry,self.coefficients)\n",
    "#         prediction = prediction.cpu().numpy()\n",
    "#         prediction = np.squeeze(prediction).T\n",
    "#         return prediction\n",
    "# \n",
    "#     def _reshape_x(self,X):\n",
    "#         return X.reshape(-1,1)\n",
    "# \n",
    "#     def _concatenate_ones(self,X):\n",
    "#         ones = np.ones(shape=X.shape[0]).reshape(-1,1)\n",
    "#         return np.concatenate((ones,X),1)\n",
    "\n",
    "# def predict_fmri_fast(train_activations, test_activations, train_fmri,use_gpu=False):\n",
    "#     \"\"\"This function fits a linear regressor using train_activations and train_fmri,\n",
    "#     then returns the predicted fmri_pred_test using the fitted weights and\n",
    "#     test_activations.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     train_activations : np.array\n",
    "#         matrix of dimensions #train_vids x #pca_components\n",
    "#         containing activations of train videos.\n",
    "#     test_activations : np.array\n",
    "#         matrix of dimensions #test_vids x #pca_components\n",
    "#         containing activations of test videos\n",
    "#     train_fmri : np.array\n",
    "#         matrix of dimensions #train_vids x  #voxels\n",
    "#         containing fMRI responses to train videos\n",
    "#     use_gpu : bool\n",
    "#         whether to use gpu or not.\n",
    "#     Returns\n",
    "#     -------\n",
    "#     fmri_pred_test: np.array\n",
    "#         matrix of dimensions #test_vids x  #voxels\n",
    "#         containing predicted fMRI responses to test videos .\n",
    "#     \"\"\"\n",
    "# \n",
    "#     reg = OLS_pytorch(use_gpu)\n",
    "#     reg.fit(train_activations,train_fmri.T)\n",
    "#     fmri_pred_test = reg.predict(test_activations)\n",
    "#     return fmri_pred_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:04:59.373172700Z",
     "start_time": "2023-12-01T15:04:59.373172700Z"
    }
   },
   "id": "491bdd0a0cbb2ade"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def get_fmri(fmri_dir, ROI):\n",
    "    \"\"\"This function loads fMRI data into a numpy array for to a given ROI.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fmri_dir : str\n",
    "        path to fMRI data. Relates to a specific subject.\n",
    "    ROI : str\n",
    "        name of ROI.\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        matrix of dimensions #train_vids x #repetitions x #voxels\n",
    "        containing fMRI responses to train videos of a given ROI\n",
    "    \"\"\"\n",
    "\n",
    "    # Loading ROI data\n",
    "    # @Julian: adjust path as soon as preprocessing is done\n",
    "    ROI_file = os.path.join(fmri_dir, ROI + \".pkl\")\n",
    "    ROI_data = load_dict(ROI_file)\n",
    "\n",
    "    # averaging ROI data across repetitions\n",
    "    ROI_data_train = np.mean(ROI_data[\"train\"], axis = 1)\n",
    "    if ROI == \"WB\":\n",
    "        voxel_mask = ROI_data['voxel_mask']\n",
    "        return ROI_data_train, voxel_mask\n",
    "\n",
    "    return ROI_data_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:04:59.912419300Z",
     "start_time": "2023-12-01T15:04:59.908942100Z"
    }
   },
   "id": "5dd959a6472e9324"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "def train_model(X_train, X_val, y_train, y_val, num_epochs = 1000):\n",
    "    \"\"\"\n",
    "    conducts the training for a particular layer, subject & ROI\n",
    "    :param X_train: training data (feature map PCs from first 800 videos from a particular layer)\n",
    "    :param X_val: validation data (feature map PCs from videos 801-900 from a particular layer)\n",
    "    :param y_train: training labels (scans for first 800 videos for the particular subject & ROI)\n",
    "    :param y_val: training labels (scans for videos 801-900 for the particular subject & ROI)\n",
    "    :param num_epochs: number of epochs for training\n",
    "    :return: model parameters & validation accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # specify number of neurons per layer, depending on the number of inputs from a particular layer and number of output voxels\n",
    "    input_neurons = X_train.shape[1]\n",
    "    hidden1_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(2/3)\n",
    "    hidden2_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(1/3)\n",
    "    output_neurons = y_train.shape[1]\n",
    "    \n",
    "    # model construction: 2 hidden layers\n",
    "    model = Sequential([\n",
    "        Dense(hidden1_neurons, input_shape=(input_neurons,), activation='relu'),\n",
    "        Dense(hidden2_neurons, activation='relu'),\n",
    "        Dense(output_neurons)\n",
    "    ])\n",
    "    \n",
    "    # Compiling the model\n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "    \n",
    "    # ToDo: define the custom evaluation metric (via keras.backend)\n",
    "    \n",
    "    # Training the model\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(X_val, y_val))\n",
    "    \n",
    "    # extract validation accuracy\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, validation_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:05:00.317726500Z",
     "start_time": "2023-12-01T15:05:00.317199400Z"
    }
   },
   "id": "101a27a9f9db4c86"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# train-val-test split, fitting linear regression model, visualize voxel predictions\n",
    "def perform_encoding(pca_dir, fmri_dir,results_dir, sub, layer, ROI = 'WB', mode = 'val', visualize_results = True, batch_size=1000):\n",
    "    if torch.cuda.is_available():\n",
    "      use_gpu = True\n",
    "    else:\n",
    "      use_gpu = False\n",
    "    \n",
    "    \n",
    "    # Load activations (PCA outputs)\n",
    "    # ToDo: adjust pca_dir to load in all scans per subject & ROI\n",
    "    # pca_dir = os.path.join(\"/content/activations\" )\n",
    "    train_activations,test_activations = get_activations(pca_dir, layer)\n",
    "    \n",
    "    # Load fMRI data (labels)\n",
    "    # ToDO: adjust directory as soon as fMRI preprocessing is done\n",
    "    if ROI == \"WB\":\n",
    "      track = \"full_track\"\n",
    "    else:\n",
    "      track = \"mini_track\"\n",
    "    fmri_dir = os.path.join(fmri_dir, track)\n",
    "    sub_fmri_dir = os.path.join(fmri_dir, sub)\n",
    "    if track == \"full_track\":\n",
    "      fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,ROI)\n",
    "    else:\n",
    "      fmri_train_all = get_fmri(sub_fmri_dir,ROI)\n",
    "    num_voxels = fmri_train_all.shape[1]\n",
    "    \n",
    "    \n",
    "    # Creating data splits\n",
    "    if mode == 'val':\n",
    "      # split labels\n",
    "      train_activations = train_activations[800:900,:]\n",
    "      val_activations = train_activations[:800,:]\n",
    "      \n",
    "      \n",
    "      fmri_train = fmri_train_all[:800,:]\n",
    "      fmri_val = fmri_train_all[800:900,:]\n",
    "      pred_fmri = np.zeros_like(fmri_val)\n",
    "      pred_fmri_save_path = os.path.join(results_dir, ROI + '_val.npy')\n",
    "      \n",
    "      trained_model, val_acc = train_model(train_activations, val_activations, fmri_train, fmri_val)\n",
    "    \n",
    "    # ToDo @Marcel: implement predictions on test set\n",
    "    # else:\n",
    "    #     fmri_train = fmri_train_all\n",
    "    #     num_test_videos = 102\n",
    "    #     pred_fmri = np.zeros((num_test_videos,num_voxels))\n",
    "    #     pred_fmri_save_path = os.path.join(results_dir, ROI + '_test.npy')\n",
    "    ######################################\n",
    "\n",
    "    # iter = 0\n",
    "    # \n",
    "    # while iter < num_voxels-batch_size:\n",
    "    #     pred_fmri[:,iter:iter+batch_size] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n",
    "    #     iter = iter+batch_size\n",
    "    # pred_fmri[:,iter:] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n",
    "    # if mode == 'val':\n",
    "    #   score = vectorized_correlation(fmri_val,pred_fmri)\n",
    "    #   ################################################\n",
    "    # \n",
    "    #   nii_save_path =  os.path.join(results_dir, ROI + '_val.nii')\n",
    "    #   ######## Result visualization ################\n",
    "    #   if track == \"full_track\" and visualize_results:\n",
    "    #       visual_mask_3D = np.zeros((78,93,71))\n",
    "    #       visual_mask_3D[voxel_mask==1]= score\n",
    "    #       brain_mask = './example.nii'\n",
    "    #       saveasnii(brain_mask,nii_save_path,visual_mask_3D)\n",
    "    #       plotting.plot_glass_brain(nii_save_path,plot_abs=False,\n",
    "    #                         title='Correlation for ' + sub+ ' and ' + layer,\n",
    "    #                         display_mode='lyr',colorbar=True,vmin=-1,vmax=1)\n",
    "    \n",
    "    ################################################\n",
    "    # return score.mean()\n",
    "    \n",
    "    # np.save(pred_fmri_save_path, pred_fmri)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:05:00.701110500Z",
     "start_time": "2023-12-01T15:05:00.685165200Z"
    }
   },
   "id": "66325d4cba2fcf93"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_activations(activations_dir, layer_name):\n",
    "    \"\"\"This function loads neural network features/activations (preprocessed using PCA) into a\n",
    "    numpy array according to a given layer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    activations_dir : str\n",
    "        Path to PCA processed Neural Network features\n",
    "    layer_name : str\n",
    "        which layer of the neural network to load,\n",
    "    Returns\n",
    "    -------\n",
    "    train_activations : np.array\n",
    "        matrix of dimensions #train_vids x #pca_components\n",
    "        containing activations of train videos\n",
    "    test_activations : np.array\n",
    "        matrix of dimensions #test_vids x #pca_components\n",
    "        containing activations of test videos\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # numpy arrays of the PCA results\n",
    "    # @ToDo: adjust after all PCAs are loaded here. Change paths & split in train & test. Currently, no test data is loaded\n",
    "    \n",
    "    train_activations = []\n",
    "    test_activations = []\n",
    "        # Loop through each file in the folder\n",
    "    for filename in os.listdir(activations_dir):\n",
    "        if filename.endswith('.pickle'):  # Consider only pickle files\n",
    "            file_path = os.path.join(activations_dir, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                loaded_data = pickle.load(file)\n",
    "                # Convert loaded data to NumPy array if needed\n",
    "                if isinstance(loaded_data, np.ndarray):\n",
    "                    # Add a new axis before appending to the list\n",
    "                    loaded_data = loaded_data[np.newaxis, ...]\n",
    "                    train_activations.append(loaded_data)\n",
    "                else:\n",
    "                    # Convert to array if data is not already in array format\n",
    "                    loaded_data = np.array(loaded_data)\n",
    "                    # Add a new axis before appending to the list\n",
    "                    loaded_data = loaded_data[np.newaxis, ...]\n",
    "                    train_activations.append(loaded_data)\n",
    "    \n",
    "    # Concatenate the data along the new axis (axis=0 for a new dimension)\n",
    "    train_activations = np.concatenate(train_activations, axis=0)\n",
    "    \n",
    "    # standardize PCA output\n",
    "    scaler = StandardScaler()\n",
    "    train_activations = scaler.fit_transform(train_activations)\n",
    "    test_activations = scaler.fit_transform(test_activations)\n",
    "\n",
    "    return train_activations, test_activations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:05:01.104680800Z",
     "start_time": "2023-12-01T15:05:01.104680800Z"
    }
   },
   "id": "4b8e62d7024d9387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3480626c7d7508c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\OneDrive - CBS - Copenhagen Business School\\Documents\\Master\\Semester3\\AdvancedML\\Brainvision_Project\\PCA\n",
      "Starting ROI:  WB sub:  sub01\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m   os\u001B[38;5;241m.\u001B[39mmakedirs(results_dir)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting ROI: \u001B[39m\u001B[38;5;124m\"\u001B[39m, ROI, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msub: \u001B[39m\u001B[38;5;124m\"\u001B[39m,sub)\n\u001B[1;32m---> 29\u001B[0m perform_encoding(pca_dir, fmri_dir,\n\u001B[0;32m     30\u001B[0m                  results_dir, sub, layer,\n\u001B[0;32m     31\u001B[0m                  ROI\u001B[38;5;241m=\u001B[39mROI,mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# mode: 'val' for model training/selection, 'test' for obtaining evaluation metrics\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompleted ROI: \u001B[39m\u001B[38;5;124m\"\u001B[39m, ROI, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msub: \u001B[39m\u001B[38;5;124m\"\u001B[39m,sub)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m----------------------------------------------------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[26], line 12\u001B[0m, in \u001B[0;36mperform_encoding\u001B[1;34m(pca_dir, fmri_dir, results_dir, sub, layer, ROI, mode, visualize_results, batch_size)\u001B[0m\n\u001B[0;32m      6\u001B[0m   use_gpu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Load activations (PCA outputs)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# ToDo: adjust pca_dir to load in all scans per subject & ROI\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# pca_dir = os.path.join(\"/content/activations\" )\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m train_activations,test_activations \u001B[38;5;241m=\u001B[39m get_activations(pca_dir, layer)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Load fMRI data (labels)\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# ToDO: adjust directory as soon as fMRI preprocessing is done\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ROI \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWB\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "Cell \u001B[1;32mIn[27], line 45\u001B[0m, in \u001B[0;36mget_activations\u001B[1;34m(activations_dir, layer_name)\u001B[0m\n\u001B[0;32m     42\u001B[0m                 train_activations\u001B[38;5;241m.\u001B[39mappend(loaded_data)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Concatenate the data along the new axis (axis=0 for a new dimension)\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m train_activations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(train_activations, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# standardize PCA output\u001B[39;00m\n\u001B[0;32m     48\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler()\n",
      "\u001B[1;31mValueError\u001B[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# define directories for PCA and fmri data (repo_root necessary for runs in Ucloud)\n",
    "repo_root = find_repo_root()\n",
    "pca_dir = os.path.join(repo_root, \"PCA\")\n",
    "print(pca_dir)\n",
    "\n",
    "fmri_dir =  os.path.join(repo_root, \"participants_data_v2021\")\n",
    "\n",
    "prediction_dir = os.path.join(repo_root, \"participants_data_v2021\")\n",
    "\n",
    "# specify layer manually: in [\"stage_1\", \"stage_2\", \"stage_3\", \"stage_4\", \"stage_5\", \"final\"]\n",
    "layer = 'stage_4'\n",
    "\n",
    "subs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]\n",
    "ROIs = [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
    "for sub in subs:\n",
    "  for ROI in ROIs:\n",
    "    if ROI == \"WB\":\n",
    "        track = \"full_track\"\n",
    "    else:\n",
    "        track = \"mini_track\"\n",
    "\n",
    "    # ToDo: implement some saving of prediction scores\n",
    "    # specify directory to save prediction scores\n",
    "    results_dir = os.path.join(prediction_dir, layer, ROI, sub)  \n",
    "    if not os.path.exists(results_dir):\n",
    "      os.makedirs(results_dir)\n",
    "    \n",
    "    print (\"Starting ROI: \", ROI, \"sub: \",sub)\n",
    "    perform_encoding(pca_dir, fmri_dir,\n",
    "                     results_dir, sub, layer,\n",
    "                     ROI=ROI,mode='val')  # mode: 'val' for model training/selection, 'test' for obtaining evaluation metrics\n",
    "    print (\"Completed ROI: \", ROI, \"sub: \",sub)\n",
    "    print(\"----------------------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T15:05:02.030927Z",
     "start_time": "2023-12-01T15:05:01.952264200Z"
    }
   },
   "id": "40072c28e4bcaf4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def calculate_vectorized_correlation(x, y):\n",
    "#     \"\"\"\n",
    "#     calculate evaluation score (per voxel)\n",
    "#     :param: x - prediction voxel activations; y - actual voxel activations\n",
    "#     :return: evaluation score, maybe other evaluation metrics can be added here as well\n",
    "#     \"\"\"\n",
    "#     dim = 0\n",
    "# \n",
    "#     centered_x = x - x.mean(axis=dim, keepdims=True)\n",
    "#     centered_y = y - y.mean(axis=dim, keepdims=True)\n",
    "# \n",
    "#     covariance = (centered_x * centered_y).sum(axis=dim, keepdims=True)\n",
    "# \n",
    "#     covariance = covariance / (x.shape[dim])\n",
    "# \n",
    "#     x_std = x.std(axis=dim, keepdims=True) + 1e-8\n",
    "#     y_std = y.std(axis=dim, keepdims=True) + 1e-8\n",
    "# \n",
    "#     corr = covariance / (x_std * y_std)\n",
    "# \n",
    "#     return corr.ravel()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d947dfc873f69b3c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
