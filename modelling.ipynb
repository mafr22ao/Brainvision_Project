{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c3b08cdd4f007a4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\julia\\anaconda3\\envs\\AdvancedML\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "from utils import find_repo_root, load_dict, calculate_vectorized_correlation, get_fmri, get_pca, correlation_metric, download_fmri\n",
    "from evaluation_utils import run_evaluation_pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.optimizers import Nadam"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T19:51:38.383872600Z",
     "start_time": "2024-02-20T19:51:18.328630300Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function Definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cef70d0abaf40bf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(layer, sub, ROI, X_train, X_val, y_train, y_val, hp_combinations,  visualize_results = True):\n",
    "    \"\"\"\n",
    "    conducts the training for a particular input layer of the CNN, subject & ROI. Saves model parameters & history\n",
    "    :param X_train: training data (feature map PCs from first 800 videos from a particular layer)\n",
    "    :param X_val: validation data (feature map PCs from videos 801-900 from a particular layer)\n",
    "    :param y_train: training labels (scans for first 800 videos for the particular subject & ROI)\n",
    "    :param y_val: training labels (scans for videos 801-900 for the particular subject & ROI)\n",
    "    :param hp_combinations: dict with all parameter values in HPO. If hpo=False in run_training_pipeline, this only contains one single set of HP values\n",
    "    :param visualize_results: whether training will be plotted\n",
    "    :return: model parameters\n",
    "    \"\"\"\n",
    "    print(\"y_train shape: \", y_train.shape)\n",
    "    print(\"Y_val shape: \", y_val.shape)\n",
    "    \n",
    "    # apply training to every HP combination. If hpo == False, only one HP combination will be checked\n",
    "    for hp_set in hp_combinations:\n",
    "        \n",
    "        # save all HP values of current iteration as variable\n",
    "        num_hidden_layers = hp_set[\"num_hidden_layers\"]\n",
    "        l2_reg = hp_set[\"l2_reg\"]\n",
    "        learning_rate = hp_set[\"learning_rate\"]\n",
    "        dropout = hp_set[\"dropout\"]\n",
    "        num_epochs = hp_set[\"num_epochs\"]\n",
    "        \n",
    "        # specify number of neurons per layer, depending on the number of inputs from a particular layer and number of output voxels\n",
    "        # leads to symmetric funnel-like shape of the network, with increasing layer sizes for WB and decreasing size for all ROIs\n",
    "        if num_hidden_layers == 1:\n",
    "            input_neurons = X_train.shape[1]\n",
    "            hidden1_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(1/2)\n",
    "            output_neurons = y_train.shape[1]\n",
    "        elif num_hidden_layers == 2:\n",
    "            input_neurons = X_train.shape[1]\n",
    "            hidden1_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(2/3)\n",
    "            hidden2_neurons = y_train.shape[1] + (X_train.shape[1] - y_train.shape[1])*(1/3)\n",
    "            output_neurons = y_train.shape[1]\n",
    "        \n",
    "        # model construction: 1 or 2 hidden layers. L2 Reg & Dropout.\n",
    "        if num_hidden_layers == 1:\n",
    "            model = Sequential([\n",
    "                Dense(hidden1_neurons, input_shape=(input_neurons,),\n",
    "                      activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(output_neurons, activation='linear',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout)\n",
    "            ])\n",
    "        elif num_hidden_layers == 2:\n",
    "            model = Sequential([\n",
    "                Dense(hidden1_neurons, input_shape=(input_neurons,),\n",
    "                      activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(hidden2_neurons, activation='relu',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout),\n",
    "                Dense(output_neurons, activation='linear',\n",
    "                      kernel_regularizer=l2(l2_reg)),\n",
    "                Dropout(dropout)\n",
    "            ])\n",
    "            \n",
    "        # early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "        # Compiling the model\n",
    "        nadam = Nadam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=nadam, loss='mean_squared_error')\n",
    "        \n",
    "        # Training the model\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=1)\n",
    "        \n",
    "        if visualize_results:\n",
    "            # print layer overview\n",
    "            print(model.summary())\n",
    "            \n",
    "            # Plot training & validation loss values\n",
    "            plt.plot(history.history['loss'])\n",
    "            plt.plot(history.history['val_loss'])\n",
    "            plt.title('Model loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "            plt.show()\n",
    "        \n",
    "        # save the model in models folder structure\n",
    "        models_dir = os.path.join(os.getcwd(), \"models\", layer, ROI, sub)\n",
    "        if not os.path.exists(models_dir):\n",
    "            os.makedirs(models_dir)\n",
    "        model_name = f\"model_hidden_{num_hidden_layers}_lr_{learning_rate}_dropout_{dropout}_l2_{l2_reg}\" + \".keras\"\n",
    "        model_path = os.path.join(models_dir, model_name)\n",
    "        print(\"Saving model...\")\n",
    "        print(model_path)\n",
    "        model.save(model_path)\n",
    "        print(\"Model saved.\")\n",
    "        \n",
    "        # save the model history\n",
    "        history_dir = os.path.join(os.getcwd(), \"model_histories\", layer, ROI, sub)\n",
    "        if not os.path.exists(history_dir):\n",
    "            os.makedirs(history_dir)\n",
    "        history_name = model_name.replace('.keras', '.history')\n",
    "        history_path = os.path.join(history_dir, history_name)\n",
    "        print(\"Saving training history...\")\n",
    "        with open(history_path, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        print(\"Training history saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "101a27a9f9db4c86"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def run_training_pipeline(hpo=False, import_type=\"direct\"):\n",
    "    \"\"\"\n",
    "    conducts training. Models will be saved in .keras format for evaluation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    hpo: if set to true, hyperparameter optimization will be conducted. Otherwise, a single set of hyperparameters will be applied. Adjust HP for that manually here.\n",
    "    import_type: set to \"direct\" if the full PCA output from the main Resnet is directly loaded into the CWD.\n",
    "                 set to \"indirect\" if a folder structure with one file per video is loaded.\n",
    "    \"\"\"\n",
    "    # load one only one main PCA file into the Ucloud session. This will determine the layer\n",
    "    layer_list = [\"stage_1\", \"stage_2\", \"stage_3\", \"stage_4\", \"stage_5\"]\n",
    "    for i in layer_list:\n",
    "        if os.path.exists(f\"{i}_pca.pkl\") or os.path.exists(f\"{i}_pca.npy\"):\n",
    "            layer = i\n",
    "            break\n",
    "\n",
    "    # all subjects and ROIs to loop through. \"WB\" is \"whole brain\"\n",
    "    subs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]\n",
    "    ROIs = [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
    "    \n",
    "    # unless already downloaded, download the fmri data\n",
    "    download_fmri()\n",
    "    \n",
    "    # Load activations (PCA outputs)\n",
    "    train_pca,val_pca = get_pca(layer, mode=\"val\", import_type=import_type)\n",
    "    \n",
    "    # specify hyperparameter grid\n",
    "    param_grid ={\"learning_rate\": [0.0001],\n",
    "                \"num_hidden_layers\": [1,2],\n",
    "                \"dropout\": [0.2, 0.4],\n",
    "                \"l2_reg\":[0.001, 0.0001, 0.00001],\n",
    "                \"num_epochs\": [100]}\n",
    "    hp_combinations = ParameterGrid(param_grid)\n",
    "    if hpo:\n",
    "        # choose hp settings to test in current session\n",
    "        hp_combinations = list(hp_combinations)[0:4]\n",
    "    else:\n",
    "        # if no HPO, specify hyperparameter settings manually here - one value per HP\n",
    "        param_grid = {\"learning_rate\": [0.0001],\n",
    "                     \"num_hidden_layers\": [2],\n",
    "                     \"dropout\": [0.2],\n",
    "                     \"l2_reg\":[0.0001],\n",
    "                     \"num_epochs\": [100]}\n",
    "        hp_combinations = ParameterGrid(param_grid)\n",
    "        \n",
    "    for sub in subs:\n",
    "      for ROI in ROIs:\n",
    "        print (\"Starting ROI: \", ROI, \"sub: \",sub)\n",
    "        if ROI == \"WB\":\n",
    "          track=\"full_track\"\n",
    "        else:\n",
    "          track=\"mini_track\"\n",
    "        # Load fMRI data\n",
    "        fmri_train, fmri_val = get_fmri(ROI, track, sub, mode=\"val\")\n",
    "    \n",
    "        # model training\n",
    "        train_model(layer=layer,\n",
    "                    sub=sub,\n",
    "                    ROI=ROI,\n",
    "                    X_train=train_pca,\n",
    "                    X_val=val_pca,\n",
    "                    y_train=fmri_train,\n",
    "                    y_val=fmri_val,\n",
    "                    hp_combinations=hp_combinations,\n",
    "                    visualize_results=True)\n",
    "        \n",
    "        print (\"Completed ROI: \", ROI, \"sub: \",sub)\n",
    "        print(\"----------------------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T10:29:55.433057400Z",
     "start_time": "2023-12-13T10:29:55.433057400Z"
    }
   },
   "id": "40072c28e4bcaf4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Training & Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e45789c2c8d7894d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# conduct training & evaluation with validation set\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_pipeline(hpo=True)\n",
    "    run_evaluation_pipeline(data_mode=\"val\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16842abb91be7006"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
