{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9faca7027604e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:41:20.747128200Z",
     "start_time": "2023-12-08T12:41:20.734363700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import pandas as pd\n",
    "from utils import  find_repo_root\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb1a356a912d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Set training_end_id to the number of samples in your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc493752a138520c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:35:54.151954600Z",
     "start_time": "2023-12-08T12:35:54.135986Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(find_repo_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a54bee-9ed4-4adf-9605-af1d76495bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d0d16c1636fca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T13:03:26.851913400Z",
     "start_time": "2023-12-08T13:03:26.842673900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: load feature maps per frame\n",
    "def load_and_combine_tensors(stage_name, input_folder, num_videos):\n",
    "    combined_tensor = []\n",
    "    video_indices = {}\n",
    "\n",
    "    for video_id in tqdm(range(1, num_videos + 1), desc=\"Loading feature maps\"):\n",
    "        filename = f\"{str(video_id).zfill(4)}_{stage_name}.pkl\"\n",
    "        file_path = os.path.join(input_folder, stage_name, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            #print(f\"Loading tensor from: {file_path}\")\n",
    "            with open(file_path, 'rb') as file:\n",
    "                tensor = pickle.load(file)\n",
    "                combined_tensor.append(tensor)\n",
    "                # Track start and end indices for each video\n",
    "                end_index = sum(t.shape[0] for t in combined_tensor)\n",
    "                video_indices[str(video_id).zfill(4)] = (end_index - tensor.shape[0], end_index)\n",
    "\n",
    "    if not combined_tensor:\n",
    "        print(\"No tensors found to combine.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_tensor = np.concatenate(combined_tensor, axis=0)\n",
    "    return combined_tensor, video_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698222bfb9b8029c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T14:05:28.167607200Z",
     "start_time": "2023-12-08T14:05:28.167607200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: globalized standardization (only based on training set)\n",
    "def flatten_tensors(combined_tensor, num_videos, frames_per_video = 30):\n",
    "    # flatten the feature maps per video\n",
    "    flattened_arrays = []\n",
    "    for i in tqdm(range(num_videos), desc=\"Flattening feature maps\"):\n",
    "        start_idx = i * frames_per_video\n",
    "        end_idx = start_idx + frames_per_video\n",
    "        video = combined_tensor[start_idx:end_idx]\n",
    "    \n",
    "        # Flatten the video and append to the list\n",
    "        flattened_arrays.append(video.flatten())\n",
    "    \n",
    "    # Convert the list of flattened arrays into a NumPy array\n",
    "    x = np.array(flattened_arrays)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def standardize_tensors(combined_tensor, training_end_id=800):\n",
    "    reshaped_tensor = flatten_tensors(combined_tensor, num_videos) # combined_tensor.reshape(combined_tensor.shape[0], -1)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    print(\"Standardizing feature maps\")\n",
    "    # Fit the scaler only on the training set\n",
    "    scaler.fit(reshaped_tensor[:training_end_id, :])\n",
    "\n",
    "    # Transform both training and test sets\n",
    "    standardized_data = scaler.transform(reshaped_tensor)\n",
    "    print(\"Feature maps standardized\")\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b6b5c20b0071fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T17:03:22.775425900Z",
     "start_time": "2023-12-08T17:03:22.774921800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Fit PCA on train, apply to train and val, save PCs, save metadata\n",
    "def apply_pca_and_save(standardized_tensor, stage_name, output_folder, training_end_id=800, n_components=400, seed=42):\n",
    "    # Fit PCA on train, apply to train and val\n",
    "    print(f\"Performing PCA on {stage_name} data...\")\n",
    "    pca = PCA(n_components=n_components,random_state=seed)\n",
    "    pca.fit(standardized_tensor[:training_end_id, :])\n",
    "    pca_tensor = pca.transform(standardized_tensor)\n",
    "    \n",
    "    # save PCs\n",
    "    pca_folder = os.path.join(output_folder, 'PCA_dataset', stage_name)\n",
    "    if not os.path.exists(pca_folder):\n",
    "        os.makedirs(pca_folder)\n",
    "    \n",
    "    pca_filename = os.path.join(pca_folder, f\"{stage_name}_pca\")\n",
    "    np.save(pca_filename,pca_tensor)\n",
    "    print(f\"PCs of {stage_name} data saved.\")\n",
    "    \n",
    "    # get metadata\n",
    "    # Debugging: Check shapes and variance\n",
    "    # print(\"PCA Tensor Shape:\", pca_tensor.shape)\n",
    "    # print(\"Explained Variance:\", np.sum(pca.explained_variance_ratio_))\n",
    "    pca_tensor_shape = pca_tensor.shape\n",
    "    variance = np.sum(pca.explained_variance_ratio_)\n",
    "    return {\n",
    "        'stage': stage_name,\n",
    "        'pca_shape': pca_tensor_shape,\n",
    "        'variance_captured': variance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda240732dae2966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T17:20:09.078116800Z",
     "start_time": "2023-12-08T17:12:11.948173900Z"
    },
    "collapsed": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading feature maps: 100%|██████████| 1000/1000 [00:23<00:00, 42.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 28, 28, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening feature maps: 100%|██████████| 1000/1000 [00:16<00:00, 59.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing feature maps\n",
      "Feature maps standardized\n",
      "(1000, 12042240)\n",
      "Performing PCA on stage_3 data...\n",
      "PCs of stage_3 data saved.\n",
      "     stage    pca_shape  variance_captured\n",
      "0  stage_3  (1000, 400)           0.632354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading feature maps: 100%|██████████| 1000/1000 [00:13<00:00, 74.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 14, 14, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening feature maps: 100%|██████████| 1000/1000 [00:09<00:00, 109.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing feature maps\n",
      "Feature maps standardized\n",
      "(1000, 6021120)\n",
      "Performing PCA on stage_4 data...\n",
      "PCs of stage_4 data saved.\n",
      "     stage    pca_shape  variance_captured\n",
      "0  stage_3  (1000, 400)           0.632354\n",
      "1  stage_4  (1000, 400)           0.629889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading feature maps: 100%|██████████| 1000/1000 [00:07<00:00, 142.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 7, 7, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening feature maps: 100%|██████████| 1000/1000 [00:04<00:00, 200.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing feature maps\n",
      "Feature maps standardized\n",
      "(1000, 3010560)\n",
      "Performing PCA on stage_5 data...\n",
      "PCs of stage_5 data saved.\n",
      "     stage    pca_shape  variance_captured\n",
      "0  stage_3  (1000, 400)           0.632354\n",
      "1  stage_4  (1000, 400)           0.629889\n",
      "2  stage_5  (1000, 400)           0.665212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading feature maps: 100%|██████████| 1000/1000 [00:00<00:00, 1340.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening feature maps: 100%|██████████| 1000/1000 [00:00<00:00, 12205.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing feature maps\n",
      "Feature maps standardized\n",
      "(1000, 30000)\n",
      "Performing PCA on final data...\n",
      "PCs of final data saved.\n",
      "     stage    pca_shape  variance_captured\n",
      "0  stage_3  (1000, 400)           0.632354\n",
      "1  stage_4  (1000, 400)           0.629889\n",
      "2  stage_5  (1000, 400)           0.665212\n",
      "3    final  (1000, 400)           0.839756\n"
     ]
    }
   ],
   "source": [
    "# Loop through all stages and perform PCA\n",
    "stages = [\"stage_1\", \"stage_3\", \"stage_4\", \"stage_5\", \"final\"] # non functioning stages: stage_2, \n",
    "input_folder = 'preprocessed_videos_30frames'\n",
    "output_folder = \"/work\"\n",
    "\n",
    "metadata = []\n",
    "for stage_name in stages:\n",
    "    \n",
    "    stage_folder = os.path.join(os.getcwd(), input_folder, stage_name)\n",
    "    num_videos = len([f for f in os.listdir(stage_folder) if os.path.isfile(os.path.join(stage_folder, f))])\n",
    "    \n",
    "    # Step 1: Load and combine the feature maps for each frame\n",
    "    combined_tensor, video_indices = load_and_combine_tensors(stage_name, input_folder, num_videos)\n",
    "    print(combined_tensor.shape)\n",
    "    \n",
    "    # Step 2: flatten feature maps per video and standardize values on training set, apply to train and val\n",
    "    standardized_tensor = standardize_tensors(combined_tensor)\n",
    "    print(standardized_tensor.shape)\n",
    "\n",
    "    del combined_tensor\n",
    "    gc.collect()\n",
    "    \n",
    "    # Step 3: Fit PCA on train, apply to train and val, save PCs, save metadata\n",
    "    stage_metadata = apply_pca_and_save(standardized_tensor, stage_name, output_folder)\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    print(metadata_df)\n",
    "    metadata_filename = os.path.join(stage_folder, f\"{stage_name}_metadata.csv\")\n",
    "    metadata_df.to_csv(metadata_filename, index=False)\n",
    "\n",
    "    del standardized_tensor\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51bd158bb567faf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "####################--------------------------------------------------------------------------------------#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85d020e-dd98-4e29-bbb1-6413211e5f27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m output_filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPCA_dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with your desired output name\u001B[39;00m\n\u001B[1;32m      4\u001B[0m output_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mgetcwd(), output_filename)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241m.\u001B[39mmake_archive(output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzip\u001B[39m\u001B[38;5;124m'\u001B[39m, directory_to_zip)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "# zip pca folder\n",
    "directory_to_zip = \"PCA_dataset\"  # Replace with your directory name\n",
    "output_filename = \"PCA_dataset\"  # Replace with your desired output name\n",
    "output_path = os.path.join(os.getcwd(), output_filename)\n",
    "shutil.make_archive(output_path, 'zip', directory_to_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7729d9ec7a47476a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch standardization for stage_2\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f820dc3863e58926"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# incremental pca for stage_2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e78aeac56aee20f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
