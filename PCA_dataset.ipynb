{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9faca7027604e2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T12:41:20.747128200Z",
     "start_time": "2023-12-08T12:41:20.734363700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import pandas as pd\n",
    "from utils import  find_repo_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.get"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1be05ebdfe909be1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set training_end_id to the number of samples in your training set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96bb1a356a912d1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marce\\PycharmProjects\\Brainvision_Project\n"
     ]
    }
   ],
   "source": [
    "print(find_repo_root())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T12:35:54.151954600Z",
     "start_time": "2023-12-08T12:35:54.135986Z"
    }
   },
   "id": "dc493752a138520c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d0d16c1636fca7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T13:03:26.851913400Z",
     "start_time": "2023-12-08T13:03:26.842673900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: load feature maps per frame\n",
    "def load_and_combine_tensors(stage_name, input_folder, num_videos):\n",
    "    combined_tensor = []\n",
    "    video_indices = {}\n",
    "\n",
    "    for video_id in range(1, num_videos + 1):\n",
    "        filename = f\"{str(video_id).zfill(4)}_{stage_name}.pkl\"\n",
    "        file_path = os.path.join(input_folder, stage_name, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            #print(f\"Loading tensor from: {file_path}\")\n",
    "            with open(file_path, 'rb') as file:\n",
    "                tensor = pickle.load(file)\n",
    "                combined_tensor.append(tensor)\n",
    "                # Track start and end indices for each video\n",
    "                end_index = sum(t.shape[0] for t in combined_tensor)\n",
    "                video_indices[str(video_id).zfill(4)] = (end_index - tensor.shape[0], end_index)\n",
    "\n",
    "    if not combined_tensor:\n",
    "        print(\"No tensors found to combine.\")\n",
    "        return None, None\n",
    "\n",
    "    combined_tensor = np.concatenate(combined_tensor, axis=0)\n",
    "    return combined_tensor, video_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698222bfb9b8029c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T14:05:28.167607200Z",
     "start_time": "2023-12-08T14:05:28.167607200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: globalized standardization (only based on training set)\n",
    "def flatten_tensors(combined_tensor, num_videos, frames_per_video = 30):\n",
    "    # flatten the feature maps per video\n",
    "    flattened_arrays = []\n",
    "    for i in range(num_videos):\n",
    "        start_idx = i * frames_per_video\n",
    "        end_idx = start_idx + frames_per_video\n",
    "        video = combined_tensor[start_idx:end_idx]\n",
    "    \n",
    "        # Flatten the video and append to the list\n",
    "        flattened_arrays.append(video.flatten())\n",
    "    \n",
    "    # Convert the list of flattened arrays into a NumPy array\n",
    "    x = np.array(flattened_arrays)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def standardize_tensors(combined_tensor, training_end_id=40):\n",
    "    reshaped_tensor = flatten_tensors(combined_tensor, num_videos) # combined_tensor.reshape(combined_tensor.shape[0], -1)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler only on the training set\n",
    "    scaler.fit(reshaped_tensor[:training_end_id, :])\n",
    "\n",
    "    # Transform both training and test sets\n",
    "    standardized_data = scaler.transform(reshaped_tensor)\n",
    "    \n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Step 3: Fit PCA on train, apply to train and val, save PCs, save metadata\n",
    "def apply_pca_and_save(standardized_tensor, stage_name, output_folder, training_end_id=40, n_components=30, seed=42):\n",
    "    # Fit PCA on train, apply to train and val\n",
    "    print(f\"Performing PCA on {stage_name} data...\")\n",
    "    pca = PCA(n_components=n_components,random_state=seed)\n",
    "    pca.fit(standardized_tensor[:training_end_id, :])\n",
    "    pca_tensor = pca.transform(standardized_tensor)\n",
    "    \n",
    "    # save PCs\n",
    "    pca_folder = os.path.join(output_folder, 'PCA_dataset', stage_name)\n",
    "    if not os.path.exists(pca_folder):\n",
    "        os.makedirs(pca_folder)\n",
    "    \n",
    "    pca_filename = os.path.join(pca_folder, f\"{stage_name}_pca\")\n",
    "    np.save(pca_filename,pca_tensor)\n",
    "    print(f\"PCs of {stage_name} data saved.\")\n",
    "    \n",
    "    # get metadata\n",
    "    # Debugging: Check shapes and variance\n",
    "    print(\"PCA Tensor Shape:\", pca_tensor.shape)\n",
    "    print(\"Explained Variance:\", np.sum(pca.explained_variance_ratio_))\n",
    "    pca_tensor_shape = pca_tensor.shape\n",
    "    variance = np.sum(pca.explained_variance_ratio_)\n",
    "    return {\n",
    "        'stage': stage_name,\n",
    "        'pca_shape': pca_tensor_shape,\n",
    "        'variance_captured': variance\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T17:03:22.775425900Z",
     "start_time": "2023-12-08T17:03:22.774921800Z"
    }
   },
   "id": "64b6b5c20b0071fe"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 1, 56, 56, 64)\n",
      "(50, 6021120)\n",
      "Performing PCA on stage_1 data...\n",
      "PCs of stage_1 data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.91285694\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "(1500, 1, 56, 56, 256)\n",
      "(50, 24084480)\n",
      "Performing PCA on stage_2 data...\n",
      "PCs of stage_2 data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.8474801\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "1  stage_2  (50, 30)           0.847480\n",
      "(1500, 1, 28, 28, 512)\n",
      "(50, 12042240)\n",
      "Performing PCA on stage_3 data...\n",
      "PCs of stage_3 data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.83511776\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "1  stage_2  (50, 30)           0.847480\n",
      "2  stage_3  (50, 30)           0.835118\n",
      "(1500, 1, 14, 14, 1024)\n",
      "(50, 6021120)\n",
      "Performing PCA on stage_4 data...\n",
      "PCs of stage_4 data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.82128096\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "1  stage_2  (50, 30)           0.847480\n",
      "2  stage_3  (50, 30)           0.835118\n",
      "3  stage_4  (50, 30)           0.821281\n",
      "(1500, 1, 7, 7, 2048)\n",
      "(50, 3010560)\n",
      "Performing PCA on stage_5 data...\n",
      "PCs of stage_5 data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.8280003\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "1  stage_2  (50, 30)           0.847480\n",
      "2  stage_3  (50, 30)           0.835118\n",
      "3  stage_4  (50, 30)           0.821281\n",
      "4  stage_5  (50, 30)           0.828000\n",
      "(1500, 1, 1000)\n",
      "(50, 30000)\n",
      "Performing PCA on final data...\n",
      "PCs of final data saved.\n",
      "PCA Tensor Shape: (50, 30)\n",
      "Explained Variance: 0.9257851\n",
      "     stage pca_shape  variance_captured\n",
      "0  stage_1  (50, 30)           0.912857\n",
      "1  stage_2  (50, 30)           0.847480\n",
      "2  stage_3  (50, 30)           0.835118\n",
      "3  stage_4  (50, 30)           0.821281\n",
      "4  stage_5  (50, 30)           0.828000\n",
      "5    final  (50, 30)           0.925785\n"
     ]
    }
   ],
   "source": [
    "# Loop through all stages and perform PCA\n",
    "stages = [\"stage_1\", \"stage_2\", \"stage_3\", \"stage_4\", \"stage_5\", \"final\"]\n",
    "input_folder = 'preprocessed_videos_30frames'\n",
    "output_folder = find_repo_root()\n",
    "\n",
    "metadata = []\n",
    "for stage_name in stages:\n",
    "    \n",
    "    stage_folder = os.path.join(os.getcwd(), input_folder, stage_name)\n",
    "    num_videos = len([f for f in os.listdir(stage_folder) if os.path.isfile(os.path.join(stage_folder, f))])\n",
    "    \n",
    "    # Step 1: Load and combine the feature maps for each frame\n",
    "    combined_tensor, video_indices = load_and_combine_tensors(stage_name, input_folder, num_videos)\n",
    "    print(combined_tensor.shape)\n",
    "    \n",
    "    # Step 2: flatten feature maps per video and standardize values on training set, apply to train and val\n",
    "    standardized_tensor = standardize_tensors(combined_tensor)\n",
    "    print(standardized_tensor.shape)\n",
    "    \n",
    "    # Step 3: Fit PCA on train, apply to train and val, save PCs, save metadata\n",
    "    stage_metadata = apply_pca_and_save(standardized_tensor, stage_name, output_folder)\n",
    "    metadata.append(stage_metadata)  \n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    print(metadata_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T17:20:09.078116800Z",
     "start_time": "2023-12-08T17:12:11.948173900Z"
    }
   },
   "id": "fda240732dae2966"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd158bb567faf7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
